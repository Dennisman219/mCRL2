\documentclass{article}
\usepackage{fullpage}
\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage[dvipsnames]{xcolor}

\setcounter{MaxMatrixCols}{10}

\font \aap cmmi10
\newcommand{\at}[1]{\mbox{\aap ,} #1}
\newcommand{\ap}{{:}}
\newcommand{\tuple}[1]{\ensuremath{\langle {#1} \rangle}}
\newcommand{\vars}{\mathit{vars}}
\newtheorem{theorem}{Theorem}
\newtheorem{acknowledgement}[theorem]{Acknowledgement}
\newtheorem{algorithm}[theorem]{Algorithm}
\newtheorem{axiom}[theorem]{Axiom}
\newtheorem{case}[theorem]{Case}
\newtheorem{claim}[theorem]{Claim}
\newtheorem{conclusion}[theorem]{Conclusion}
\newtheorem{condition}[theorem]{Condition}
\newtheorem{conjecture}[theorem]{Conjecture}
\newtheorem{corollary}[theorem]{Corollary}
\newtheorem{criterion}[theorem]{Criterion}
\newtheorem{definition}[theorem]{Definition}
\newtheorem{example}[theorem]{Example}
\newtheorem{exercise}[theorem]{Exercise}
\newtheorem{lemma}[theorem]{Lemma}
\newtheorem{notation}[theorem]{Notation}
\newtheorem{problem}[theorem]{Problem}
\newtheorem{proposition}[theorem]{Proposition}
\newtheorem{remark}[theorem]{Remark}
\newtheorem{solution}[theorem]{Solution}
\newtheorem{summary}[theorem]{Summary}
\newenvironment{proof}[1][Proof]{\noindent\textbf{#1.} }{\ \rule{0.5em}{0.5em}}

\newcommand{\up}{\blacktriangle}
\newcommand{\down}{\blacktriangledown}
\begin{document}

\title{PBES instantiation and solving}
\author{Wieger Wesselink, Tim Willemse}
\maketitle

This document describes instantiation and solving algorithms for PBESs that
are used in the tools \textbf{pbesinst} and \textbf{pbessolve}.

\section{Finite algorithm}

In this section we describe an implementation of the finite instantiation
algorithm \textsc{PbesInstFinite} that eliminates data parameters with
finite sorts. It is implemented in the tool \textbf{pbesinst}. Let $\mathcal{%
E=(\sigma }_{1}X_{1}(d_{1}:D_{1},e_{1}:E_{1})=\varphi _{1})\cdots \mathcal{%
(\sigma }_{n}X_{n}(d_{n}:D_{n},e_{n}:E_{n})=\varphi _{n})$ be a PBES. We
assume that all data sorts $D_{i}$ are finite and all data sorts $E_{i}$ are
infinite. Let $r$ be a data rewriter, and let $\rho $ be an injective
function that creates a unique predicate variable from a predicate variable
name and a data value according to $\rho (X(d:D,e:E),d_{0})\rightarrow Y(e:E)
$, where $D$ is finite and $E$ is infinite and $d_{0}\in D$. Note that $D$
and $D_{i}$ may be multi-dimensional sorts.%
\begin{equation*}
\begin{array}{l}
\text{\textsc{PbesInstFinite(}}\mathcal{E}\text{, }r\text{, }\rho \text{%
\textsc{)}} \\
\text{\textbf{for }}i:=1\cdots n\text{ \textbf{do}} \\
\qquad \mathcal{E}_{i}:=\{\mathcal{\sigma }_{i}\rho (X_{i},d)=R(\varphi
_{k}[d_{k}:=d])\ |\ d\in D_{i}\} \\
\text{\textbf{return }}\mathcal{E}_{1}\cdots \mathcal{E}_{n},%
\end{array}%
\end{equation*}%
with $R$ a rewriter on pbes expressions that is defined as follows:%
\begin{eqnarray*}
R(b) &=&b \\
R(\lnot \varphi ) &=&\lnot R(\varphi ) \\
R(\varphi \oplus \psi ) &=&R(\varphi )\oplus R(\psi ) \\
R(X_{i}(d,e)) &=&\left\{
\begin{array}{cc}
\rho (X_{i},r(d))(r(e)) & \text{if }FV(d)=\emptyset  \\
\bigvee\limits_{d_{i}\in D_{i}}r(d=d_{i})\wedge \rho (X_{i},d_{i})(r(e)) &
\text{if }FV(d)\neq \emptyset
\end{array}%
\right.  \\
R(\forall _{d:D}.\varphi ) &=&\forall _{d:D}.R(\varphi ) \\
R(\exists _{d:D}.\varphi ) &=&\exists _{d:D}.R(\varphi )
\end{eqnarray*}%
where $\oplus \in \{\vee ,\wedge ,\Rightarrow \}$, $b$ a data expression and
$\varphi $ and $\psi $ pbes expressions and $FV(d)$ is the set of free
variables appearing in $d$.\newpage

\section{Lazy algorithm}

In this section we describe an implementation of the lazy instantiation
algorithm \textsc{PbesInstLazy} that uses instantiation to compute a BES. It
is implemented in the tool \textbf{pbesinst}. It takes two extra parameters,
an injective function $\rho $ that renames proposition variables to
predicate variables, and a rewriter $R$ that eliminates quantifiers from
predicate formulae. Let $\mathcal{E=(\sigma }_{1}X_{1}(d_{1}:D_{1})=\varphi
_{1})\ldots \mathcal{(\sigma }_{n}X_{n}(d_{n}:D_{n})=\varphi _{n})$ be a
PBES, and $X_{init}(e_{init})$ an initial state.%
\begin{equation*}
\begin{array}{l}
\text{\textsc{PbesInstLazy(}}\mathcal{E}\text{, }X_{init}(e_{init})\text{, }R%
\text{, }\rho \text{\textsc{)}} \\
\text{\textbf{for }}i:=1\cdots n\text{ \textbf{do }}\mathcal{E}%
_{i}:=\epsilon  \\
todo:=\{R(X_{init}(e_{init}))\} \\
done:=\emptyset  \\
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\
\qquad done:=done\cup \{X_{k}(e)\} \\
\qquad X^{e}:=\rho (X_{k}(e)) \\
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\
\qquad \mathcal{E}_{k}:=\mathcal{E}_{k}(\mathcal{\sigma }_{k}X^{e}=\rho
(\psi ^{e})) \\
\qquad todo:=todo\cup \{Y(f)\in \mathsf{occ}(\psi ^{e})\ |\ Y(f)\notin done\}
\\
\text{\textbf{return }}\mathcal{E}_{1}\cdots \mathcal{E}_{n},%
\end{array}%
\end{equation*}%
where $\rho $ is extended from predicate variables to quantifier free
predicate formulae using

\begin{eqnarray*}
\rho (b) &=&b \\
\quad \rho (\varphi \oplus \psi ) &=&\rho (\varphi )\oplus \rho (\psi )
\end{eqnarray*}%
\newpage

\section{Generic lazy algorithms}

In this section two generic variants of lazy PBES instantiation are
described that report all discovered BES equations using a callback
function \textsc{ReportEquation}. These versions are later extended to compute
structure graphs.

The first version \textsc{PbesInstLazy1}
maintains a collection $done$, that contains all BES variables for
which an equation has been computed.%
\begin{equation*}
\begin{array}{l}
\text{\textsc{PbesInstLazy1(}}\mathcal{E}\text{, }X_{init}(e_{init})\text{, }%
R\text{\textsc{)}} \\
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
done:=\emptyset \\
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\
\qquad done:=done\cup \{X_{k}(e)\} \\
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\
\qquad \text{\textsc{ReportEquation}}(X_{k}(e),\psi ^{e}) \\
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus done)
\end{array}
\end{equation*}

The second version \textsc{PbesInstLazy2} maintains a set $discovered$
instead of $done$. This set contains BES
variables that have been discovered, but for which the corresponding
equation may not have been computed yet. The sets are related via
$done = discovered \setminus todo$.

\begin{equation*}
\begin{array}{l}
\text{\textsc{PbesInstLazy2(}}\mathcal{E}\text{, }X_{init}(e_{init})\text{, } R\text{\textsc{)}} \\
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\
\qquad \text{\textsc{ReportEquation}}(X_{k}(e),\psi ^{e}) \\
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e})
\end{array}
\end{equation*}

It turned out that the second version has slightly better performance for
some larger use cases, so the second version is implemented in the tool \textbf{pbessolve}.

To support breadth first and depth first search, the implementation stores the
set $todo$ as a double ended queue. New elements are always appended to $todo$.
In the case of breadth first search always the first element is chosen, and in the
case of depth first search the last element.

\section{Structure graphs}
A structure graph is a tuple $(V,E,d,r)$ with $V$ a set of nodes containing BES variables,
$E$ a set of edges, $r:V\rightarrow \mathbb{N}$ a partial function that assigns a rank to each node,
and $d:V\rightarrow \{\blacktriangle ,\blacktriangledown ,\top ,\bot \}$ a partial function that
assigns a decoration to each node. A structure graph is formally defined using the
following SOS rules:

\begin{equation*}
\frac{X\in bnd(\mathcal{E})}{r(X) = rank_{\mathcal{E}}(X)}
\end{equation*}

\begin{equation*}
\frac{{}}{d(true) = \top }\qquad \frac{{}}{d(false) = \bot }
\end{equation*}
\begin{equation*}
\frac{{}}{d(f \wedge f^{\prime}) = \blacktriangle } \qquad
\frac{{}}{d(f \vee f^{\prime}) = \blacktriangledown }
\end{equation*}
\begin{equation*}
\frac{f\blacktriangle \quad f\rightarrow g}{(f\wedge f^{\prime })\rightarrow g} \qquad
\frac{f\blacktriangle \quad f\rightarrow g}{(f^{\prime }\wedge f)\rightarrow g}
\end{equation*}%
\begin{equation*}
\frac{f\blacktriangledown \quad f\rightarrow g}{(f\vee f^{\prime })\rightarrow g} \qquad
\frac{f\blacktriangledown \quad f\rightarrow g}{(f^{\prime }\vee f)\rightarrow g}
\end{equation*}%
\begin{equation*}
\frac{\lnot f\blacktriangle }{f\wedge f^{\prime }\rightarrow f}\qquad \frac{%
\lnot f^{\prime }\blacktriangle }{f\wedge f^{\prime }\rightarrow f^{\prime }}
\end{equation*}%
\begin{equation*}
\frac{\lnot f\blacktriangledown }{f\vee f^{\prime }\rightarrow f}\qquad
\frac{\lnot f^{\prime }\blacktriangledown }{f\vee f^{\prime }\rightarrow
f^{\prime }}
\end{equation*}%
\begin{equation*}
\frac{{}}{X\wedge f\rightarrow X}\qquad \frac{{}}{f\wedge X\rightarrow X}
\end{equation*}%
\begin{equation*}
\frac{{}}{X\vee f\rightarrow X}\qquad \frac{{}}{f\vee X\rightarrow X}
\end{equation*}%
\begin{equation*}
\frac{\sigma X=f\wedge f^{\prime }\in \mathcal{E}}{d(X) = \blacktriangle }\qquad
\frac{\sigma X=f\vee f^{\prime }\in \mathcal{E}}{d(X) = \blacktriangledown }
\end{equation*}%
\begin{equation*}
\frac{\sigma X=Y\in \mathcal{E}}{X\rightarrow Y}\qquad \frac{\sigma X=\top
\in \mathcal{E}}{X\rightarrow \top }\qquad \frac{\sigma X=\bot \in \mathcal{E%
}}{X\rightarrow \bot }
\end{equation*}%
\begin{equation*}
\frac{\sigma X=f\wedge f^{\prime }\in \mathcal{E}\quad f\wedge f^{\prime
}\rightarrow g}{X\rightarrow g}
\end{equation*}%
\begin{equation*}
\frac{\sigma X=f\vee f^{\prime }\in \mathcal{E}\quad f\vee f^{\prime
}\rightarrow g}{X\rightarrow g}
\end{equation*}%
Note that in this definition separate nodes are created for the left hand
side $X$ and the right hand side $f$ of each equation $\sigma X=f$ . This is
undesirable, hence in implementations usually the nodes $X$ and $f$ are
merged into one node labeled with $X$.

\subsection{Attractor sets}
Let $A \subseteq V$ be a subset of vertices of a structure graph $G=(V,E,d,r)$. We
define the following algorithms for computing an attractor set of $A$. The value
$\alpha = 0$ corresponds with disjunction and $\alpha = 1$ with conjunction.

\begin{equation*}
\begin{array}{l}
\text{\textsc{Attr(}}A,\alpha \text{\textsc{)}} \\ 
todo:=\bigcup\limits_{u\in A}\left( pred(u)\setminus A\right)  \\ 
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\ 
\qquad \text{\textbf{choose }}u\in todo \\ 
\qquad todo:=todo\ \backslash \ \{u\} \\ 
\qquad \text{\textbf{if} }d(u) = \alpha \vee succ(u)\subseteq A\text{ \textbf{then}} \\ 
\qquad \qquad A:=A\cup \{u\} \\ 
\qquad \qquad todo:=todo\cup \left( pred(u)\setminus A\right)  \\ 
\text{\textbf{return }}A%
\end{array}%
\end{equation*}%
where%
\begin{eqnarray*}
pred(v) &=&\{u\in V\mid (u,v)\in E\} \\
succ(u) &=&\{v\in V\mid (u,v)\in E\}
\end{eqnarray*}%

\subsection{Recursive procedure for solving structure graphs}

Let $G=(V,E,d,r)$ be a structure graph. The following algorithm is used to compute
a partitioning of $V$ into $\left( W_{0},W_{1}\right) $ of vertices $W_{0}$ that
represent equations evaluating to true and vertices $W_{1}$ that represent equations
evaluating to false. A precondition of this algorithm is that it contains no nodes with
decoration $\top $ or $\bot$. This algorithm is based on Zielonka's recursive algorithm.
\begin{equation*}
\begin{array}{l}
\textsc{SolveRecursive}(V) \\ 
\text{\textbf{if} }V=\emptyset \text{ \textbf{then return }}(\emptyset
,\emptyset ) \\ 
m:=\min (\{r(v)\mid v\in V\}) \\ 
\alpha :=m \mod 2 \\ 
U:=\{v\in V\mid r(v)=m\} \\ 
A:=\text{\textsc{Attr}}\left( U,\alpha \right)  \\ 
W_{0}^{\prime },W_{1}^{\prime }:=\text{\textsc{SolveRecursive(}}V\setminus A%
\text{)} \\ 
\textbf{if } W_{1-\alpha }^{\prime }= \emptyset \textbf{ then} \\ 
\qquad W_{\alpha },W_{1-\alpha }:=A\cup W_{\alpha }^{\prime }, \emptyset \\ 
\text{\textbf{else}} \\ 
\qquad B:=\text{\textsc{Attr}}\left( W_{1-\alpha }^{\prime },1-\alpha \right)  \\ 
\qquad W_{0},W_{1}:=\text{\textsc{SolveRecursive(}}V\setminus B\text{)} \\ 
\qquad W_{1-\alpha }:=W_{1-\alpha }\cup B \\ 
\text{\textbf{return }}W_{0},W_{1}%
\end{array}%
\end{equation*}%
where%
\begin{equation*}
succ(u,U)=\left\{ 
\begin{array}{ll}
succ(u)\cap U & \text{if }succ(u)\cap U\neq \emptyset  \\ 
succ(u) & \text{otherwise}%
\end{array}%
\right. 
\end{equation*}

Tom van Dijk has introduced an optimization that may reduce the number of recursive calls. Note that this optimized version does not compute a complete strategy, so it
cannot be used for counter example generation(!)

\begin{equation*}
\begin{array}{l}
\textsc{SolveRecursive}(V) \\ 
\text{\textbf{if} }V=\emptyset \text{ \textbf{then return }}(\emptyset
,\emptyset ) \\ 
m:=\min (\{r(v)\mid v\in V\}) \\ 
\alpha :=m \mod 2 \\ 
U:=\{v\in V\mid r(v)=m\} \\ 
A:=\text{\textsc{Attr}}\left( U,\alpha \right)  \\ 
W_{0}^{\prime },W_{1}^{\prime }:=\text{\textsc{SolveRecursive(}}V\setminus A%
\text{)} \\
\colorbox{lightgray}{$
B:=\text{\textsc{Attr}}\left( W_{1-\alpha }^{\prime },1-\alpha \right) $}\\ 
\colorbox{lightgray}{$
\text{\textbf{if} }W_{1-\alpha }^{\prime }=B\text{ \textbf{then}} $}\\ 
\colorbox{lightgray}{$
\qquad W_{\alpha },W_{1-\alpha }:=A\cup W_{\alpha }^{\prime },B $}\\ 
\colorbox{lightgray}{$
\text{\textbf{else}} $}\\ 
\colorbox{lightgray}{$
\qquad W_{0},W_{1}:=\textsc{SolveRecursive}(V \setminus B)$}\\ 
\colorbox{lightgray}{$
\qquad W_{1-\alpha }:=W_{1-\alpha }\cup B $}\\ 
\text{\textbf{return }}W_{0},W_{1}
\end{array}
\end{equation*}
The algorithm can be extended to sets $V$ containing nodes with decoration $%
\top $ or $\bot $ as follows:%
\begin{equation*}
\begin{array}{l}
\textsc{SolveRecursiveExtended}(V) \\ 
V_{1}:=\textsc{Attr}\left( \{v\in V\mid d(v) = \bot \},1\right) \\ 
V_{0}:=\textsc{Attr}\left( \{v\in V\mid d(v) = \top \},0\right) \\ 
\left( W_{0},W_{1}\right) :=\text{\textsc{SolveRecursive(}}V\setminus
(V_{0}\cup V_{1})\text{\textsc{)}} \\ 
\text{\textbf{return }}\left( W_{0}\cup V_{1},W_{0}\cup V_{1}\right)%
\end{array}
\end{equation*}
Another possible optimization of the \textsc{SolveRecursive} algorithm is to insert the
following shortcuts. This doesn't seem to have much effect in practice, so currently it
isn't enabled in the code.
\begin{equation*}
\begin{array}{l}
\textsc{SolveRecursive}(V) \\ 
\text{\textbf{if} }V=\emptyset \text{ \textbf{then return }}(\emptyset
,\emptyset ) \\ 
m:=\min (\{r(v)\mid v\in V\}) \\ 
\alpha :=m \bmod 2 \\ 
U:=\{v\in V\mid r(v)=m\} \\ 
\colorbox{lightgray}{$
\textbf{if }h=m\wedge even(m)\text{ \textbf{then return} }(\emptyset,V) $}\\ 
\colorbox{lightgray}{$
\textbf{if }h=m\wedge odd(m)\text{ \textbf{then return} }(V,\emptyset) $}\\
A:=\text{\textsc{Attr}}\left( U,\alpha \right)  \\ 
W_{0}^{\prime },W_{1}^{\prime }:=\text{\textsc{SolveRecursive(}}V\setminus A%
\text{)} \\ 
\textbf{if } W_{1-\alpha }^{\prime }= \emptyset \textbf{ then} \\ 
\qquad W_{\alpha },W_{1-\alpha }:=A\cup W_{\alpha }^{\prime }, \emptyset \\ 
\text{\textbf{else}} \\ 
\qquad B:=\text{\textsc{Attr}}\left( W_{1-\alpha }^{\prime },1-\alpha \right)  \\ 
\qquad W_{0},W_{1}:=\text{\textsc{SolveRecursive(}}V\setminus B\text{)} \\ 
\qquad W_{1-\alpha }:=W_{1-\alpha }\cup B \\ 
\text{\textbf{return }}W_{0},W_{1}%
\end{array}%
\end{equation*}%


\subsection{Computing a winning strategy}

The \textsc{SolveRecursive} algorithm can as a side effect produce a mapping $\tau$ that
corresponds to a winning strategy, by slightly adapting the algorithm and the attractor set computation:

\begin{equation*}
\begin{array}{l}
precondition: V \subseteq dom(r) \cup dom(d) \\
\text{\textsc{SolveRecursive(}}V\text{\textsc{)}} \\ 
\text{\textbf{if} }V=\emptyset \text{ \textbf{then return }}(\emptyset
,\emptyset ) \\ 
m:=\min (\{r(v)\mid v\in V\}) \\ 
\alpha :=m \mod 2 \\ 
U:=\{v\in V\mid r(v)=m\} \\ 
\colorbox{lightgray}{$
\text{\textbf{for} }u\in U\text{ \textbf{if} }d(u)=\alpha \wedge succ(u)\neq
\emptyset \text{ \textbf{then }}\tau \lbrack u]:=v\text{ \textbf{with} }v\in
succ(u) $}\\ 
A:=\text{\textsc{Attr}}\left( U,\alpha \right)  \\ 
W_{0}^{\prime },W_{1}^{\prime }:=\text{\textsc{SolveRecursive(}}V\setminus A%
\text{)} \\ 
B:=\text{\textsc{Attr}}\left( W_{1-\alpha }^{\prime },1-\alpha \right)  \\ 
\text{\textbf{if} }W_{1-\alpha }^{\prime }=B\text{ \textbf{then}} \\ 
\qquad W_{\alpha },W_{1-\alpha }:=A\cup W_{\alpha }^{\prime },B \\ 
\text{\textbf{else}} \\ 
\qquad W_{0},W_{1}:=\text{\textsc{SolveRecursive(}}V\setminus B\text{)} \\ 
\qquad W_{1-\alpha }:=W_{1-\alpha }\cup B \\ 
\text{\textbf{return }}W_{0},W_{1}
\end{array}
\end{equation*}
with

\begin{equation*}
\begin{array}{l}
\text{\textsc{Attr(}}A,\alpha \text{\textsc{)}} \\ 
todo:=\bigcup\limits_{u\in A}\left( pred(u)\setminus A\right)  \\ 
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\ 
\qquad \text{\textbf{choose }}u\in todo \\ 
\qquad todo:=todo\ \backslash \ \{u\} \\ 
\qquad \text{\textbf{if} }d(u) = \alpha \vee succ(u)\subseteq A\text{ \textbf{then}} \\ 
\qquad \qquad \colorbox{lightgray}{$%
\text{\textbf{if} }d(u)\neq 1 - \alpha \text{ \textbf{then }}\tau
\lbrack u]:=v\text{ \textbf{with} }v\in A\cap succ(u) $}\\ 
\qquad \qquad A:=A\cup \{u\} \\ 
\qquad \qquad todo:=todo\cup \left( pred(u)\setminus A\right)  \\ 
\text{\textbf{return }}A%
\end{array}%
\end{equation*}%
Note that the mapping $\tau$ is a global variable of \textsc{Attr}, which is ugly.
In the implementation the strategy is an attribute of the nodes.

\section{Structure graph based PBES instantiation}

In the tool \textbf{pbessolve} an extension called \textsc{%
PbesInstStructureGraph} of the \textsc{PbesInstLazy2} algorithm is
implemented that builds a structure graph from the reported equations. On
top of it several optimizations to this algorithm are defined. For
readability, we only present the full algorithms here, and highlight the
changes with respect to previous versions. A graph $G$ is
represented as a tuple $(V,E)$ with $V$ the set of vertices and $E$ the set
of edges.%
\begin{equation*}
\begin{array}{l}
\text{\textsc{PbesInstStructureGraph(}}\mathcal{E}\text{, }X_{init}(e_{init})%
\text{, }R\text{\textsc{)}} \\
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
\colorbox{lightgray}{$%
(V,E) :=(\emptyset ,\emptyset ) $}\\
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\
\qquad \colorbox{lightgray}{$%
(V,E) := (V,E) \cup SG^{0}(X_{k}(e),\psi ^{e}) $}\\
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\text{\textbf{return }}G%
\end{array}%
\end{equation*}

where $SG^{0}$ and $SG^{1}$ are defined as%
\begin{equation*}
\begin{array}{|c|c|}
\hline
\psi  & SG^{0}(\varphi ,\psi ) \\ \hline\hline
true & (\{\varphi \},\emptyset ) \\ \hline
false & (\{\varphi \},\emptyset ) \\ \hline
Y & (\{\varphi ,\psi \},\{(\varphi ,\psi )\}) \\ \hline
\psi _{1}\wedge \cdots \wedge \psi _{n} & \left( \{\varphi ,\psi _{1},\cdots
,\psi _{n}\},\{(\varphi ,\psi _{1}),\cdots ,(\varphi ,\psi _{n})\}\right)
\cup \bigcup\limits_{i=1}^{n}SG^{1}(\psi _{i}) \\ \hline
\psi _{1}\vee \cdots \vee \psi _{n} & \left( \{\varphi ,\psi _{1},\cdots
,\psi _{n}\},\{(\varphi ,\psi _{1}),\cdots ,(\varphi ,\psi _{n})\}\right)
\cup \bigcup\limits_{i=1}^{n}SG^{1}(\psi _{i}) \\ \hline
\end{array}%
\end{equation*}%
\begin{equation*}
\begin{array}{|c|c|}
\hline
\psi  & SG^{1}(\varphi ) \\ \hline\hline
true & (\{\psi \},\emptyset ) \\ \hline
false & (\{\psi \},\emptyset ) \\ \hline
Y & (\{\psi \},\emptyset ) \\ \hline
\psi _{1}\wedge \cdots \wedge \psi _{n} & \left( \{\psi ,\psi _{1},\cdots
,\psi _{n}\},\{(\psi ,\psi _{1}),\cdots ,(\psi ,\psi _{n})\}\right) \cup
\bigcup\limits_{i=1}^{n}SG^{1}(\psi _{i}) \\ \hline
\psi _{1}\vee \cdots \vee \psi _{n} & \left( \{\psi ,\psi _{1},\cdots ,\psi
_{n}\},\{(\psi ,\psi _{1}),\cdots ,(\psi ,\psi _{n})\}\right) \cup
\bigcup\limits_{i=1}^{n}SG^{1}(\psi _{i}), \\ \hline
\end{array}%
\end{equation*}%
where we assume that in $\psi _{1}\wedge \cdots \wedge \psi _{n}$ none of
the $\psi _{i}$ is a conjunction, and in $\psi _{1}\vee \cdots \vee \psi _{n}
$ none of the $\psi _{i}$ is a disjunction. Note that both $SG^{0}(\varphi
,\psi )$ and $SG^{1}(\varphi ,\psi )$ are defined as a pair ($V$, $E$) of
nodes and edges.

\subsection{Optimisation~1}

The lemma below indicates that one can simplify the BES equation that is being created
without affecting the solution to the BES. 
\begin{lemma}
The solution to all variables in a BES $\mathcal{E} (\sigma X = f) \mathcal{E}'$ is 
equivalent to the solution to those variables in the BES
$\mathcal{E} (\sigma X = f[X := b_\sigma]) \mathcal{E}'$, where
$b_\sigma = true$ if $\sigma = \nu$ and $b_\sigma = false$ if $\sigma = \mu$.
\end{lemma}
Using this lemma, rather than creating a structure graph underlying the
equation $\sigma X_e = \psi^e$, we can create a structure graph
for $\sigma X_e = \psi^e[X^e := b_\sigma]$.
This can be done by adding the following assignment below the assignment
$\psi^e := R(\varphi_k[d_k := e])$:
\[
\psi^e := R(\psi^e[X^e := b_\sigma])
\]
This leads to the following adaptations in the code:

\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_1$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \colorbox{lightgray}{$%
\psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false]) \textbf{ else }  R(\psi^e[X^e := true]) $}\\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\text{\textbf{return }}G,%
\end{array}%
\end{equation*}%


\subsection{Optimisation~2}

Our second optimisation exploits the fact that some of the BES
equations that are generated while exploring the PBES are already
solved (possibly after using optimisation~1).  We first introduce
some additional notation. Let $(V,E)$ be a (partial) structure graph
underlying the PBES $\mathcal{E}$.  By $S_0$ we denote the set of
vertices that represent equations with solution $true$, whereas
$S_1$ denotes the set of vertices representing equations with
solution $false$.  Let $\pi$ be a partial
function that maps vertices to the propositional variables they
represent (and only those vertices that represent propositional variables). 
For a set of vertices $S \subseteq V$, we define
the substitution $\rho_i$ as follows for all $s \in S \cap \textsf{dom}(\pi)$: 
$\rho_i(\pi(s)) = true$ if $i = 0$ and $\rho_i(\pi(s)) = false$ if
$i = 1$. The union of two substitutions is again a substitution, provided
that the domain of variables these substitutions range over are disjoint.

The lemma below indicates how one can utilise such information to simplify the BES equation 
that is being created, again without affecting the solution to the BES.

\begin{lemma}
The solution to all variables in a BES $\mathcal{F} \equiv \mathcal{E} (\sigma X = f) \mathcal{E}'$ is 
equivalent to the solution to those variables in the BES
$\mathcal{F}' \equiv \mathcal{E} (\sigma X = f(\rho_0(S_0) \cup \rho_1(S_1))) \mathcal{E}'$, where
for all $S_0 \cup S_1 \subseteq \{v \in V \mid \forall \theta,\theta':
\lbrack \mathcal{F} \rbrack \theta(\pi(v) = 
\lbrack \mathcal{F} \rbrack \theta(\pi(v)\}$, where $\lbrack \mathcal{F} \rbrack \theta$
denotes the solution to $\mathcal{F}$ under environment $\theta$.
\end{lemma}
Using this lemma, rather than creating a structure graph underlying the
equation $\sigma X_e = \psi^e$, we can create a structure graph
for $\sigma X_e = \psi^e(\rho_0(S_0) \cup \rho_1(S_1))$, provided that
$S_0$ and $S_1$ contain vertices that represent solved equations. 
This leads to the following adaptations in the code: we maintain
two sets $S_0$ and $S_1$ for which we can (cheaply) establish that these
correspond to solved equations, and we add an assignment below the assignment
of optimisation~1.

\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_2$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
\colorbox{lightgray}{$%
S_0:= \emptyset $}\\
\colorbox{lightgray}{$%
S_1:= \emptyset $}\\
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\ 
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false])
\textbf{ else }  R(\psi^e[X^e := true]) \\
\qquad \colorbox{lightgray}{$%
\psi^{e} := R(\psi^e (\rho_0(S_0) \cup \rho_1(S_1) ) ) $}\\
\qquad \colorbox{lightgray}{$%
S_0 := S_0 \cup \{ X^e \mid \psi^e \equiv true \} $}\\
\qquad \colorbox{lightgray}{$%
S_1 := S_1 \cup \{ X^e \mid \psi^e \equiv false \} $}\\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\text{\textbf{return }}G,%
\end{array}%
\end{equation*}%

\paragraph{Note on computing winning strategies.} Rewriting the propositional formula
$\psi^e$ using $S_0$ and $S_1$ may result in a loss of information, preventing us from
constructing a winning strategy for both players. We can solve that by mimicking the
attractor set computation in our rewriting; that is, we implement $R(\psi^e(\rho_0(S_0) \cup \rho_1(S_1)))$
using a rewriter $R^+$ which takes a formula (and implicitly takes sets $S_0$ and $S_1$ into account. 
This leads to the following bottom-up
procedure in which $R^+(f)$ yields a tuple $(b, f')$, where $b$ is either a Boolean value,
or the value $\bot$, and $f'$ is a propositional formula that is equivalent to $f$ under
the assumption that $S_0$ and $S_1$ are solved.

\begin{itemize}
\item Case $f \equiv true$  then $R^+(f) = (f,f)$;
\item Case $f \equiv false$ then $R^+(f) = (f,f)$;
\item Case $f \equiv X$; then $R^+(f) =(true,X)$ when $X \in S_0$, $(false, X)$ when $X \in S_1$ and $(\bot, X)$ otherwise;
\item Case $f \equiv f_1 \wedge f_2$, assuming that $R^+(f_i) = (b_i,f'_i)$. If $b_1 = b_2 = true$, then
$R^+(f) = (true, f_1' \wedge f_2')$. If $b_1 = false$ and $b_2 \neq false$, then $R^+(f) = (false, f_1')$. 
If $b_2 = false$ and $b_1 \neq false$ then $R^+(f) = (false,f_2')$. If $b_1 = b_2 = false$ then
$R^+(f) = (false, f_1')$ when $|f_1'| < |f_2'|$ and $(false,f_2')$ otherwise. If $b_1 = b_2 = \bot$ then
$R^+(f) = (\bot, f_1' \wedge f_2')$;

\item Case $f \equiv f_1 \vee f_2$,  assuming that $R^+(f_i) = (b_i,f'_i)$. If $b_1 = b_2 = false$, then
$R^+(f) = (false, f_1' \vee f_2')$. If $b_1 = true$ and $b_2 \neq true$, then $R^+(f) = (true, f_1')$. 
If $b_2 = true$ and $b_1 \neq true$ then $R^+(f) = (true,f_2')$. If $b_1 = b_2 = true$ then
$R^+(f) = (true, f_1')$ when $|f_1'| < |f_2'|$ and $(true,f_2')$ otherwise. If $b_1 = b_2 = \bot$ then
$R^+(f) = (\bot, f_1' \vee f_2')$.
\end{itemize}

The assignment $\psi^e := R(\psi^e(\rho_0(S_0) \cup \rho_1(S_1)))$ can be replaced by an assignment
$(b,\psi^e) := R^+(\psi^e)$, and the extension to $S_0$ and $S_1$ can then be replaced by the
following code:
\[
\begin{array}{l}
\textbf{if $b = true$ then } S_0 := S_0 \cup \{ X^e \} \\
\textbf{if $b = false$ then } S_1 := S_1 \cup \{ X^e \} \\
\end{array}
\]
Note that $R^+$ can probably also be implemented at the level of PBES expressions.

This results in the following adapted version:
\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_{2.1}$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
\colorbox{lightgray}{$%
S_0:= \emptyset $}\\
\colorbox{lightgray}{$%
S_1:= \emptyset $}\\
\text{\textbf{while }}todo\neq \emptyset \text{ \textbf{do}} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\ 
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false])
\textbf{ else }  R(\psi^e[X^e := true]) \\
\qquad \colorbox{lightgray}{$%
(b,\psi^e) := R^+(\psi^e) $}\\
\qquad \colorbox{lightgray}{$%
\textbf{if $b = true$ then } S_0 := S_0 \cup \{ X^e \} $}\\
\qquad \colorbox{lightgray}{$%
\textbf{if $b = false$ then } S_1 := S_1 \cup \{ X^e \} $}\\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\text{\textbf{return }}(V, E)
\end{array}%
\end{equation*}%

\subsection{Optimisation~3}


When the computation of $SG^0(\varphi,\psi,r)$ finishes and the subgraph represented by $SG^0$ 
is effectively solved (which is the case if it represents $true$ or $false$), we can use these
results to solve other variables by propagating the information in $S^0$ and $S^1$ to
the structure graph constructed so far. The modification is minor, re-using the
attractor set computation (and setting of a winning strategy) that is also part of Zielonka's 
recursive algorithm. More
specifically, we can ensure that $S_0$ and $S_1$ are closed under the appropriate
attractor set computations. Furthermore, if the initial vertex belongs to either
$S_0$ or $S_1$, we can terminate the search. This leads to the following modified algorithm:

\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_{3}$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
S_0:= \emptyset \\
S_1:= \emptyset \\
\textbf{while }todo\neq \emptyset%
\colorbox{lightgray}{$%
\wedge X_{init}(e_{init}) \notin S_0 \cup S_1 $}%
\textbf{ do} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\ 
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false])
\textbf{ else }  R(\psi^e[X^e := true]) \\
\qquad (b,\psi^e) := R^+(\psi^e) \\
\qquad \textbf{if $b = true$ then } S_0 := \colorbox{lightgray}{$%
\textsc{Attr}(S_0 \cup \{ X^e \}, 0) $}\\
\qquad \textbf{if $b = false$ then } S_1 := \colorbox{lightgray}{$%
\textsc{Attr}(S_1 \cup \{ X^e \}, 1) $}\\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\colorbox{lightgray}{$
\textbf{if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_0 \textbf{ then }
$}\\
\colorbox{lightgray}{$
\qquad V := V \setminus \textsc{Attr}(todo, 1)
$}\\
\colorbox{lightgray}{$
\textbf{else if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_1 \textbf{ then }
$}\\
\colorbox{lightgray}{$
\qquad V := V \setminus \textsc{Attr}(todo, 0)
$}\\
\textbf{return }(V, E)
\end{array}%
\end{equation*}%

Further needless instantiation can be avoided by utilising the attractor
strategies that are set when extending $S_0$ and $S_1$ by, once in a while, computing
which vertices are still reachable from $X_\textit{init}(e_\textit{init})$, ignoring
edges emanating from a vertex that are not part of the strategy for that vertex (if the
strategy is set). This reachability analysis may be combined with a cheap algorithm that
detects whether the game can in fact already be solved (see e.g. optimisation 4). 

\paragraph{Note on computing winning strategies.} The winning strategy can be set by extending
it using the attractor strategy that is computed while
computing $Attr_0(S_0 \cup \{X^e\})$ and $Attr_1(S_1 \cup \{X^e\})$.

\subsection{Optimisation 4}
Several simple algorithms exist that can solve partial structure graphs. An example
algorithm is, for instance, the algorithm that computes whether is an
\emph{odd-rank} dominated conjunctive loop or an \emph{even-rank} dominated disjunctive loop.
The \texttt{pbes2bool} optimisation, implemented at the level of the structure graph is as
follows (it typically assumes that the set $U$ contains all fully explored vertices, $r$ is
the rank function, $\textsf{dom}(r)$ yields the set of vertices with a rank associated to
them; the function $\mathsf{parity}(p) = \up$ when $p$ is odd, and $\down$ otherwise).
%\begin{equation*}
%\begin{array}{l}
%\text{\textsc{FindLoop}}(U,v,w,p,\text{visited})\\  %w is reachable from v
%\textbf{if } w = v \textbf{ then return } true \\
%\textbf{if } v \text{ is underfined } \textbf{then } v := w \\
%\textbf{if } d(w) \in\{\top,\bot\} \textbf{ then return } false \\
%\textbf{if } w \in \mathsf{dom}(r) \text{ and } r(w) \neq p \textbf{ then return } false \\
%\textbf{if } w \in \mathsf{keys}(\text{visited}) \textbf{ then return } \text{visited}(w) \\
%\textbf{if } w \in U \textbf{ then }\\
%\qquad \text{visited}(w) := false\\
%\qquad \textbf{if } (w \in \mathsf{dom}(d) \Rightarrow d(w) = \textsf{parity}(p)) \textbf{ then }\\
%\qquad \qquad b := \bigvee\limits_{u \in \{ u' \mid w \to u' \}} \textsc{FindLoop}(U,v, u, p, \text{visited}) \\
%\qquad \textbf{else } \\
%\qquad \qquad b := \bigwedge\limits_{u \in \{ u' \mid w \to u' \}} \textsc{FindLoop}(U,v, u, p, \text{visited}) \\
%\qquad \text{visited}(w) := b\\
%\qquad \textbf{return } b\\
%%\textbf{return $false$}
%\end{array}
%\end{equation*}

\begin{equation*}
\begin{array}{l}
\text{\textsc{FindLoop}}(U,v,w,p,\text{visited})\\  %w is reachable from v
\textbf{if } d(w) \in\{\top,\bot\} \textbf{ then return } false \\
\textbf{if } w \in \mathsf{dom}(r) \text{ and } r(w) \neq p \textbf{ then return } false \\
\textbf{if } w \in \mathsf{keys}(\text{visited}) \textbf{ then return } \text{visited}(w) \\
\textbf{if } w \in U \textbf{ then }\\
\qquad \text{visited}(w) := false\\
\qquad \textbf{if } (w \in \mathsf{dom}(d) \Rightarrow d(w) = \textsf{parity}(p)) \textbf{ then }\\
\qquad \qquad b := (w \to v) \vee \bigvee\limits_{u \in \{ u' \mid w \to u' \wedge u' \neq v \}} \textsc{FindLoop}(U,v, u, p, \text{visited}) \\
\qquad \textbf{else } \\
\qquad \qquad \textbf{return } false \\
%\qquad \qquad b := \bigwedge\limits_{u \in \{ u' \mid w \to u' \wedge u' \neq v \}} \textsc{FindLoop}(U,v, u, p, \text{visited}) \\
\qquad \text{visited}(w) := b\\
\qquad \textbf{return } b\\
\textbf{return $false$}
\end{array}
\end{equation*}


The routine $\textsc{FindLoop}(U,v,w,r(v),\text{visited})$ finds a
$r(v)$-ranked (possibly tree-like) loop starting in $v$, within a set of vertices
$U$. Note that in the above routine, parameter $w$ represents the `current' vertex,
whereas $v$ represents the vertex from which the search was initiated.  Note that 
parameter $p$ fulfils the role
of the fixpoint sign $\sigma$ in the original algorithm, parameter $v$ fulfils
the role of $X_k(e)$ and $w$ fulfils the role of $\phi$. 

The procedure $\textsc{FindLoops}$ can be called from within the main algorithm; it searches for loops in the structure graph currently constructed.
\begin{equation*}
\begin{array}{l}
\text{\textsc{FindLoops}}(discovered, todo, S_0, S_1)\\  %w is reachable from v
done := discovered \setminus todo \\
\text{visited} := [] \\
b_0,b_1 := false,false \\
\textbf{for } u \in done \cap \textsf{dom}(r) \textbf{ do} \\
\qquad \textbf{if } u \notin \textsf{keys}(\text{visited}) \textbf{ then } \text{visited}(u) := false\\
\qquad b := \textsc{FindLoop}(done, u, u, r(u), \text{visited}) \\
\qquad \text{visited}(u) := b\\
\qquad \textbf{if } b \textbf{ then } \\
\qquad \qquad \textbf{if } r(u) \bmod 2 = 0 \textbf{ then } S_0,b_0 := S_0 \cup \{u\}, true\\
\qquad\qquad \textbf{else } S_1,b_1 := S_1 \cup \{u\}, true\\
\textbf{if } b_0 \textbf{ then } S_0 := \textsc{Attr}(S_0, 0)\\ 
\textbf{if } b_1 \textbf{ then } S_1 := \textsc{Attr}(S_1, 1)\\ 
\textbf{return } S_0, S_1
\end{array}
\end{equation*}
The above optimisation can be integrated in the algorithm as follows:

\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_{4}$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
S_0:= \emptyset \\
S_1:= \emptyset \\
\textbf{while }todo \neq \emptyset \wedge X_{init}(e_{init}) \notin S_0 \cup S_1 \textbf{ do} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\ 
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false])
\textbf{ else }  R(\psi^e[X^e := true]) \\
\qquad (b,\psi^e) := R^+(\psi^e) \\
\qquad \textbf{if $b = true$ then } S_0 := \textsc{Attr}(S_0 \cup \{ X^e \}, 0) \\
\qquad \textbf{if $b = false$ then } S_1 := \textsc{Attr}(S_1 \cup \{ X^e \}, 1) \\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\qquad \colorbox{lightgray}{$%
S_0, S_1 := \text{\textsc{FindLoops}}(discovered, todo, S_0, S_1) $}\\  %w is reachable from v
\textbf{if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_0 \textbf{ then }\\
\qquad V := V \setminus \textsc{Attr}(todo, 1) \\
\textbf{else if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_1 \textbf{ then }\\
\qquad V := V \setminus \textsc{Attr}(todo, 0) \\
\text{\textbf{return }}(V, E)
\end{array}%
\end{equation*}%

Note that in the call to \textsc{FindLoops} we would like to pass the argument $done = discovered \setminus todo$.
However this set is not available, and it is too expensive to compute. So we pass the larger set $discovered$, and we use $discovered \cap dom(r) = discovered \setminus todo$.

\subsection{Optimisation 5} 

A generalisation of optimisation~4 utilises a so-called \emph{fatal attractor}. Fatal attractors are based on the
observation that those vertices with a dominant priority that are attracted into themselves are won by the player
with the parity of this dominant priority. A simple algorithm exploiting this is the following. It uses a modified
attractor computation.

\begin{equation*}
\begin{array}{l}
\text{\textsc{FatalAttractors}}(V, S_0, S_1)\\
J := \{ j \mid \exists u \in V \cap dom(r) : r(u) = j \} \\
\textbf{for } j \in J \\
\qquad \alpha := j \bmod 2\\
\qquad U_j := \{u \in V \mid r(u) = j \wedge (\alpha = 0 \Rightarrow d(u) \neq false) 
\wedge (\alpha = 1 \Rightarrow d(u) \neq true) \} \setminus S_{1 - \alpha} \\
\qquad U := U_j \cup S_\alpha\\
\qquad X := \textsc{AttrMinRank}(U, \alpha, V, j) \\
\qquad Y := V \setminus \textsc{Attr}(V \setminus X, 1-\alpha) \\
\qquad \textbf{while } X \neq Y \textbf{ do }\\
\qquad \qquad X := \textsc{AttrMinRank}(U \cap Y, \alpha, V, j) \\
\qquad \qquad Y := Y \setminus \textsc{Attr}(Y \setminus X, 1-\alpha) \\
\qquad S_\alpha := S_\alpha \cup X \\
\textbf{return } \textsc{Attr}(S_0, 0), \textsc{Attr}(S_1, 1)
\end{array}
\end{equation*}
where $\textsc{AttrMinRank}$ is a slightly modified version of the original attractor
set computation $\textsc{Attr}$, where only predecessors in $U$ with a rank of at least
$j$ are considered:
\begin{equation*}
\begin{array}{l}
\text{\textsc{AttrMinRank}}(A, \alpha, U, j) \\
todo := \bigcup_{u \in A} (%
\colorbox{lightgray}{$ pred^{\geq j}(u, U) $}%
\setminus A) \\
\textbf{while } todo \neq \emptyset \textbf{ do} \\
\qquad \textbf{choose } u \in todo \\
\qquad todo := todo \setminus \{u\} \\
\qquad \textbf{if } d(u) = \alpha \vee succ(u) \subseteq A \\
\qquad \qquad A := A \cup \{u \} \\
\qquad \qquad todo := todo \cup (%
\colorbox{lightgray}{$ pred^{\geq j}(u, U) $}%
\setminus A)  \\
\textbf{return } A
\end{array}
\end{equation*}

where
\begin{eqnarray*}
pred^{\geq j}(u, U) = \{v \in U \mid (v,u) \in E \wedge (r(v) \ge j \vee (v \notin dom(r) \wedge d(v) \in \{\blacktriangle,\blacktriangledown\}) \}
\end{eqnarray*}%

The above optimisation can be integrated in the algorithm as follows:

\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_{5}$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
S_0:= \emptyset \\
S_1:= \emptyset \\
\textbf{while }todo \neq \emptyset \wedge X_{init}(e_{init}) \notin S_0 \cup S_1 \textbf{ do} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\ 
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false])
\textbf{ else }  R(\psi^e[X^e := true]) \\
\qquad (b,\psi^e) := R^+(\psi^e) \\
\qquad \textbf{if $b = true$ then } S_0 := \textsc{Attr}(S_0 \cup \{ X^e \}, 0) \\
\qquad \textbf{if $b = false$ then } S_1 := \textsc{Attr}(S_1 \cup \{ X^e \}, 1) \\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\qquad \colorbox{lightgray}{$%
S_0, S_1 := \text{\textsc{FatalAttractors}}(V, S_0, S_1) $}\\  %w is reachable from v
\textbf{if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_0 \textbf{ then }\\
\qquad V := V \setminus \textsc{Attr}(todo, 1) \\
\textbf{else if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_1 \textbf{ then }\\
\qquad V := V \setminus \textsc{Attr}(todo, 0) \\
\text{\textbf{return }}(V, E)
\end{array}%
\end{equation*}%

\subsection{Optimisation 6}
Optimisation 6 is a slightly differen fatal attractor computation that is very close to the
original one by Michael Huth, Jim Huan-Pu Kuo, and Nir Piterman.

\begin{equation*}
\begin{array}{l}
\text{\textsc{AttrMinRankOriginal}}(A, \alpha, U, j) \\
\{\textit{compute the $\alpha$-min attractor into the set $A$, restricted to vertices in $U$}\}\\
todo := \bigcup_{u \in A} (%
pred^{\geq j}(u, U) ) \\
X := todo \cap A \\
\textbf{while } todo \neq \emptyset \textbf{ do} \\
\qquad \textbf{choose } u \in todo \\
\qquad todo := todo \setminus \{u\} \\
\qquad \textbf{if } d(u) = \alpha \vee succ(u) \subseteq A \cup X \\
\qquad \qquad X := X \cup \{u \} \\
\qquad \qquad todo := todo \cup (%
pred^{\geq j}(u, U) \setminus X)  \\
\textbf{return } X
\end{array}
\end{equation*}

\begin{equation*}
\begin{array}{l}
\text{\textsc{FatalAttractorsOriginal}}(V, S_0, S_1)\\
J := \{ j \mid \exists u \in V : r(u) = j \} \\
\textbf{for } j \in J \\
\qquad \alpha := j \mod 2\\
\qquad U_j := \{u \in V \mid r(u) = j \wedge (\alpha = 0 \Rightarrow d(u) \neq false) 
\wedge (\alpha = 1 \Rightarrow d(u) \neq true) \} \setminus S_{1-\alpha} \\
\qquad X := \emptyset \\
\qquad \textbf{while } U_j \neq \emptyset \wedge U_j \neq X \\ 
\qquad \qquad X := U_j\\
\qquad \qquad Y := \textsc{AttrMinRankOriginal}(X \cup S_\alpha, \alpha, V, j) \\
\qquad \qquad \textbf{if } U_j \subseteq Y \\
\qquad \qquad \qquad S_\alpha := S_\alpha \cup Y \\
\qquad \qquad \qquad break \\
\qquad \qquad \textbf{else }\\
\qquad \qquad \qquad U_j := U_j \cap Y\\
\textbf{return } \textsc{Attr}(S_0, 0), \textsc{Attr}(S_1, 1)
\end{array}
\end{equation*}

\subsection{Optimisation 7}
In optimisation 7 the sets $S_0$ and $S_1$ are extended by solving a partial game.

\begin{equation*}
\begin{array}{l}
\textsc{PbesinstStructureGraph$_{7}$(}\mathcal{E}, X_{init}(e_{init}), R) \\ 
init:=R(X_{init}(e_{init})) \\
todo:=\{init\} \\
discovered:=\{init\} \\
(V,E) :=(\emptyset ,\emptyset ) \\ 
S_0:= \emptyset \\
S_1:= \emptyset \\
\textbf{while }todo \neq \emptyset \wedge X_{init}(e_{init}) \notin S_0 \cup S_1 \textbf{ do} \\ 
\qquad \text{\textbf{choose }}X_{k}(e)\in todo \\ 
\qquad todo:=todo\ \backslash \ \{X_{k}(e)\} \\ 
\qquad \psi ^{e}:=R(\varphi _{k}[d_{k}:=e]) \\ 
\qquad \psi^e := \textbf{ if } \sigma_k = \mu \textbf{ then } R(\psi^e[X^e := false])
\textbf{ else }  R(\psi^e[X^e := true]) \\
\qquad (b,\psi^e) := R^+(\psi^e) \\
\qquad \textbf{if $b = true$ then } S_0 := \textsc{Attr}(S_0 \cup \{ X^e \}, 0) \\
\qquad \textbf{if $b = false$ then } S_1 := \textsc{Attr}(S_1 \cup \{ X^e \}, 1) \\
\qquad (V,E) := (V,E) \cup SG^{0}(X^{e},\psi ^{e}) \\ 
\qquad todo:=todo\cup (\mathrm{occ}(\psi ^{e})\setminus discovered) \\
\qquad discovered:=discovered\ \cup \mathrm{occ}(\psi ^{e}) \\
\qquad \colorbox{lightgray}{$
S_0, S_1 := \textsc{PartialSolve}(V,todo,S_0,S_1) $}\\
\textbf{if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_0 \textbf{ then }\\
\qquad V := V \setminus \textsc{Attr}(todo, 1) \\
\textbf{else if } todo \neq \emptyset \land X_{init}(e_{init}) \in S_1 \textbf{ then }\\
\qquad V := V \setminus \textsc{Attr}(todo, 0) \\
\text{\textbf{return }}(V, E)
\end{array}
\end{equation*}
where \textsc{PartialSolve} is defined as

\begin{equation*}
\begin{array}{l}
\textsc{PartialSolve}(V,todo,S_0,S_1)\\
\{\textit{use the solver to compute an over and underapproaximation}\}\\
S_0,S_1 := \textsc{Attr}(S_0, 0), \textsc{Attr}(S_1,1) \\
(W_0,W_1) := \textsc{SolveRecursive}(V \setminus (S_1 \cup \textsc{Attr}(S_0 \cup todo,0)))\\
S_1 := \textsc{Attr}(S_1 \cup W_1,1) \\
(W_0,W_1) := \textsc{SolveRecursive}(V \setminus (S_0 \cup \textsc{Attr}(S_1 \cup todo,1)))\\
S_0 := \textsc{Attr}(S_0 \cup W_0,0) \\
\end{array}
\end{equation*}

\end{document}
